{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterfactual Regret Minimization with Kuhn Poker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import NumPy and define constants for the number of cards and number of possible actions. In our algorithm each player only has two possible actions: check and bet. This simplifies our code. This is possible because a fold can be treated as a check, and a call can be treated as a bet without loss of generality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N_ACTIONS = 2 # 0 = check, 1 = bet\n",
    "N_CARDS = 3 # 0 = jack, 1 = queen, 2 = king\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Vanilla CFR each information set is visited multiple times each iteration. InformationSet stores self.regret_sum, self.strategy_sum, self.strategy, self.reach_pr, and self.reach_pr_sum. self.regret_sum, self.strategy_sum, and self.strategy are arrays indexed by an action. In our algorithm check is index 0 and bet is index 1. self.regret_sum is the sum of counterfactual regrets for each action over all visits. self.strategy_sum is the sum of each visit’s strategy multiplied by the information set player’s reach probability. self.regret_sum is used to calculate the next strategy to try. self.strategy_sum and self.reach_pr_sum is used to calculate the average strategy. self.strategy is the strategy for the current iteration and self.reach_pr accumulates the probability of reaching an information set. As more iterations are performed, average regret will minimize and the average strategy will converge to a Nash equilibrium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformationSet():\n",
    "    \"\"\"\n",
    "    Classe che rappresenta un information set.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        # key = history + carta del giocatore\n",
    "        self.key = key\n",
    "        # array contentente la somma dei counterfactual regrets per ogni azione in tutte le visite dell'information set. \n",
    "        # Check = index 0, Bet = index 1\n",
    "        self.regret_sum = np.zeros(N_ACTIONS) \n",
    "        # array contentente la somma della strategie di ogni visita moltiplicata per la reach probability del player corrente\n",
    "        self.strategy_sum = np.zeros(N_ACTIONS) \n",
    "        self.strategy = np.repeat(1/N_ACTIONS, N_ACTIONS) # array contentente la strategia corrente (inizializzata a uniforme)\n",
    "        self.reach_pr = 0 # reach probability del player corrente\n",
    "        self.reach_pr_sum = 0\n",
    "\n",
    "    def next_strategy(self):\n",
    "        \"\"\"\n",
    "        Aggiorna la strategia corrente e la somma delle strategie.\n",
    "        \"\"\"\n",
    "        self.strategy_sum += self.reach_pr * self.strategy\n",
    "        self.strategy = self.calc_strategy()\n",
    "        self.reach_pr_sum += self.reach_pr\n",
    "        self.reach_pr = 0\n",
    "\n",
    "    def calc_strategy(self):\n",
    "        \"\"\"\n",
    "        Calcola la strategia corrente a partire dai counterfactual regrets.\n",
    "        Sceglie una strategia proporzionale ai agli elementi positivi del regret_sum facendo il Regret Matching.\n",
    "        \"\"\"\n",
    "        \n",
    "        strategy = np.where(self.regret_sum > 0, self.regret_sum, 0) # prende solo i regret positivi\n",
    "\n",
    "        total = sum(strategy) # somma dei regret positivi\n",
    "        if total > 0: \n",
    "            strategy = strategy / total # normalizza i regret positivi\n",
    "        else:\n",
    "            n = N_ACTIONS\n",
    "            strategy = np.repeat(1/n, n) # se non ci sono regret positivi, sceglie una strategia uniforme\n",
    "\n",
    "        return strategy\n",
    "\n",
    "    def get_average_strategy(self):\n",
    "        \"\"\"\n",
    "        Calculate average strategy over all iterations. This is the\n",
    "        Nash equilibrium strategy.\n",
    "        \"\"\"\n",
    "        strategy = self.strategy_sum / self.reach_pr_sum\n",
    "\n",
    "        # Purify to remove actions that are likely a mistake\n",
    "        strategy = np.where(strategy < 0.001, 0, strategy)\n",
    "\n",
    "        # Re-normalize\n",
    "        total = sum(strategy)\n",
    "        strategy /= total\n",
    "\n",
    "        return strategy\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        strategies = ['{:03.2f}'.format(x)\n",
    "                      for x in self.get_average_strategy()]\n",
    "        return '{} {}'.format(self.key.ljust(6), strategies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function recursively performs a depth-first traverse across the game tree. In Vanilla CFR we traverse the entire game tree every iteration. cfr() performs different tasks depending on if the node is a chance node, a terminal node, or a decision node. It takes seven parameters.\n",
    "\n",
    "- i_map is the hash map of information sets.\n",
    "history is a string that represents our current location in the game tree. Each character represents an action we have taken and an edge we have followed.\n",
    "- ‘r’ is a random chance event.\n",
    "- ‘c’ is a check action.\n",
    "- ‘b’ is a bet action.\n",
    "- card_1 is the private card of player one.\n",
    "- card_2 is the private card of player two.\n",
    "- pr_1 is player one’s contribution to the reach probability—the probability that we’ve reached history.\n",
    "- pr_2 is player two’s contribution to the reach probability.\n",
    "- pr_c is the contribution of chance events to the reach probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cfr(i_map, history=\"\", card_1=-1, card_2=-1, pr_1=1, pr_2=1, pr_c=1):\n",
    "    \"\"\"\n",
    "    Counterfactual regret minimization algorithm.\n",
    "\n",
    "    Parameteri\n",
    "    ----------\n",
    "    i_map: dizionario degli information set\n",
    "    history : [{'r', 'c', 'b'}], stringa che rappresenta il path preso nel game tree\n",
    "        'r': random chance action\n",
    "        'c': check action\n",
    "        'b': bet action\n",
    "    card_1 : carta del player 1\n",
    "    card_2 : carta del player 2\n",
    "    pr_1 : Probabilità che il player 1 arrivi a history\n",
    "    pr_2 : Probabilità che il player 2 arrivi a history\n",
    "    pr_c: Contributo di probabilità del chance node per raggiungere history\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Se l'history è un chance node, allora ritorna il valore atteso\n",
    "    if is_chance_node(history):\n",
    "        return chance_util(i_map)\n",
    "    \n",
    "    # Se l'history è un terminal node, allora ritorna la terminal utility di questa combinazione di carte\n",
    "    if is_terminal(history):\n",
    "        return terminal_util(history, card_1, card_2)\n",
    "\n",
    "    # --------------Se arrivo qui, allora l'history è un decision node----------------\n",
    "    \n",
    "    # Calcolo il numero di azioni che sono state prese fino ad ora\n",
    "    n = len(history)    \n",
    "    # Se n è pari, allora è il turno del player 1, altrimenti è il turno del player 2\n",
    "    is_player_1 = n % 2 == 0\n",
    "    # Deduco l'infromation set della history del player corrente\n",
    "    info_set = get_info_set(i_map, card_1 if is_player_1 else card_2, history)\n",
    "    # Deduco la strategia corrente del player corrente\n",
    "    strategy = info_set.strategy\n",
    "    # Aggiungo la reach probability del player corrente all'information set\n",
    "    if is_player_1:\n",
    "        info_set.reach_pr += pr_1\n",
    "    else:\n",
    "        info_set.reach_pr += pr_2\n",
    "\n",
    "    # Counterfactual utility per azione\n",
    "    action_utils = np.zeros(N_ACTIONS)\n",
    "\n",
    "    # Chiamo ricorsivamente cfr per ogni azione possibile  \n",
    "    for i, action in enumerate([\"c\", \"b\"]):\n",
    "        next_history = history + action\n",
    "        if is_player_1:\n",
    "            action_utils[i] = -1 * cfr(i_map= i_map, history= next_history,\n",
    "                                       card_1= card_1, card_2= card_2,\n",
    "                                       pr_1= pr_1 * strategy[i], pr_2= pr_2, pr_c= pr_c) \n",
    "        else:\n",
    "            action_utils[i] = -1 * cfr(i_map = i_map, history= next_history,\n",
    "                                       card_1= card_1,card_2= card_2,\n",
    "                                       pr_1= pr_1,pr_2= pr_2 * strategy[i],pr_c= pr_c)\n",
    "\n",
    "    # Calcolo i counterfactual regrets\n",
    "    util = sum(action_utils * strategy) # somma delle utility delle azioni pesate con la strategia corrente\n",
    "    regrets = action_utils - util # calcolo i counterfactual regrets\n",
    "    # Aggiorno i counterfactual regrets dell'information set\n",
    "    if is_player_1:\n",
    "        info_set.regret_sum += pr_2 * pr_c * regrets\n",
    "    else:\n",
    "        info_set.regret_sum += pr_1 * pr_c * regrets\n",
    "    \n",
    "    return util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def card_str(card):\n",
    "    \"\"\"\n",
    "    Ritorna una string representation delle carte.\n",
    "    \"\"\"\n",
    "    if card == 0:\n",
    "        return \"J\"\n",
    "    elif card == 1:\n",
    "        return \"Q\"\n",
    "    return \"K\"\n",
    "\n",
    "\n",
    "def get_info_set(i_map, card, history):\n",
    "    \"\"\"\n",
    "    Ritorna l'information set associato alla carta e alla history.\n",
    "    Se non esiste, allora lo crea e lo aggiunge al dizionario.\n",
    "    \"\"\"\n",
    "    # La chiave è composta da una stringa che rappresenta la carta e la history\n",
    "    key = card_str(card) + \" \" + history\n",
    "    info_set = None\n",
    "    \n",
    "    # Se la chiave non è presente nel dizionario, allora creo un nuovo information set\n",
    "    if key not in i_map:\n",
    "        # Creo un'istanza della classe InformationSet\n",
    "        info_set = InformationSet(key)\n",
    "        # Aggiungo il nuovo information set al dizionario\n",
    "        i_map[key] = info_set\n",
    "        return info_set\n",
    "    \n",
    "    # Altrimenti ritorno l'information set associato alla chiave\n",
    "    return i_map[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is_chance_node() determines if we are at a chance node by checking for an empty history. \n",
    "This works because in Kuhn Poker chance events only occur at the start of the game. If is_chance_node() returns true then chance_util() enumerates all possible combinations of chance nodes. There are six total possible combinations. For each possibility, we recursively call cfr(). The next history is \"rr\" to represent two random chance actions. Neither player has taken any actions, so their reach probabilities are 1. Each chance event has a uniformly random probability of occurring of 1/n_possibilities. The expected value over all possibilities is just the sum of the utility of each possibility divided by the number of possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chance_node(history):\n",
    "    return history == \"\" # return True if history == \"\" else False\n",
    "\n",
    "# se sono in un chance node\n",
    "def chance_util(i_map):\n",
    "    expected_value = 0\n",
    "    n_possibilities = 6\n",
    "    # 6 possibili combinazioni di carte\n",
    "    for i in range(N_CARDS):\n",
    "        for j in range(N_CARDS):\n",
    "            if i != j:\n",
    "                # ottengo l'ev di ogni combinazione\n",
    "                expected_value += cfr(i_map= i_map,\n",
    "                                      history= \"rr\",\n",
    "                                      card_1= i,\n",
    "                                      card_2= j,\n",
    "                                      pr_1= 1, # probabilità che il giocatore 1 arrivi a quel nodo è 1 perchè ancora non ha giocato\n",
    "                                      pr_2= 1, # probabilità che il giocatore 2 arrivi a quel nodo è 1 perchè ancora non ha giocato\n",
    "                                      pr_c= 1/n_possibilities) # probabilità che il chance node arrivi a quel nodo è 1/6 perchè è uan distribuzione uniforme\n",
    "    return expected_value/n_possibilities # ritorno la media degli ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_terminal(history):\n",
    "    \"\"\"\n",
    "    Ritorna True se l`history` è una terminal history.\n",
    "    \"\"\"    \n",
    "    # 5 possibili terminal history\n",
    "    possibilities = {\"rrcc\": True, # showdown con check e check\n",
    "                     \"rrbb\": True, # showdown con bet e bet(call)                  \n",
    "                     \"rrcbb\": True, # showdown con check, bet e bet(call)\n",
    "                     \"rrbc\": True, # non showdown con bet e check(fold)\n",
    "                     \"rrcbc\": True, # non showdown con check, bet e check(fold)\n",
    "                     }\n",
    "    return history in possibilities\n",
    "\n",
    "\n",
    "def terminal_util(history, card_1, card_2):\n",
    "    \"\"\"\n",
    "    Ritorna l'utility di una terminal history per il player corrente. \n",
    "    Siccome i players is alternano ad ogni turno, se il numero di azioni è pari allora\n",
    "    il player corrente è il player 1 altrimenti è il player 2 \n",
    "    \"\"\"\n",
    "    n = len(history) # numero di azioni fatte\n",
    "    # se il numero di azioni è pari allora è il player 1 altrimenti è il player 2\n",
    "    card_player = card_1 if n % 2 == 0 else card_2 \n",
    "    card_opponent = card_2 if n % 2 == 0 else card_1\n",
    "\n",
    "    # No showdown\n",
    "    if history == \"rrcbc\" or history == \"rrbc\":\n",
    "        # Opponent ha foldato, il player corrente vince 1\n",
    "        return 1\n",
    "    # Showdown senza bets\n",
    "    elif history == \"rrcc\":\n",
    "        # Chi ha la carta più alta vince 1\n",
    "        return 1 if card_player > card_opponent else -1\n",
    "\n",
    "    # Showdown con bet e call\n",
    "    if history == \"rrcbb\" or history == \"rrbb\":\n",
    "        # Chi ha la carta più alta vince 2\n",
    "        return 2 if card_player > card_opponent else -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(ev, i_map):\n",
    "    print('player 1 expected value: {}'.format(ev))\n",
    "    print('player 2 expected value: {}'.format(-1 * ev))\n",
    "\n",
    "    print()\n",
    "    print('player 1 strategies:')\n",
    "    print(f\"History  Check    Bet\")\n",
    "    sorted_items = sorted(i_map.items(), key=lambda x: x[0])\n",
    "    for _, v in filter(lambda x: len(x[0]) % 2 == 0, sorted_items):\n",
    "        print(v)\n",
    "    print()\n",
    "    print('player 2 strategies:')\n",
    "    print(f\"History  Check    Bet\")\n",
    "    for _, v in filter(lambda x: len(x[0]) % 2 == 1, sorted_items):\n",
    "        print(v)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each iteration of cfr() returns the expected game value for that iteration. We are able to approximate the true game value by averaging over all iterations. The true game value is the expected amount that a player will win while following the average strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 expected value: -0.05666979368427769\n",
      "player 2 expected value: 0.05666979368427769\n",
      "\n",
      "player 1 strategies:\n",
      "History  Check    Bet\n",
      "J rr   ['0.79', '0.21']\n",
      "J rrcb ['1.00', '0.00']\n",
      "K rr   ['0.39', '0.61']\n",
      "K rrcb ['0.00', '1.00']\n",
      "Q rr   ['1.00', '0.00']\n",
      "Q rrcb ['0.45', '0.55']\n",
      "\n",
      "player 2 strategies:\n",
      "History  Check    Bet\n",
      "J rrb  ['1.00', '0.00']\n",
      "J rrc  ['0.67', '0.33']\n",
      "K rrb  ['0.00', '1.00']\n",
      "K rrc  ['0.00', '1.00']\n",
      "Q rrb  ['0.66', '0.34']\n",
      "Q rrc  ['1.00', '0.00']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i_map = {}   # dizionario degli information set\n",
    "n_iterations = 10000 # numero di iterazioni\n",
    "expected_game_value = 0\n",
    "\n",
    "for _ in range(n_iterations): # run CFR algorithm n_iterations times\n",
    "    expected_game_value += cfr(i_map) # update game value\n",
    "    for _, v in i_map.items():\n",
    "        v.next_strategy() # update strategy\n",
    "\n",
    "expected_game_value /= n_iterations\n",
    "\n",
    "display_results(expected_game_value, i_map) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the program for 100,000 iterations it will produce a slightly better approximation. Some CFR implementations have run for up to 6 million core hours [6]. As you approach an infinite number of iterations the overall average regret will approach zero and the average strategy will approach a Nash equilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategies indicate that if player one has a J her first move should be to check 79% of the time and bet 21% of the time. If player two has a K and player one checked, he should bet 100% of the time. This is just one Nash equilibrium. Kuhn Poker has infinitely many. Vanilla CFR is a deterministic algorithm, but many variants are non-deterministic. A non-deterministic CFR may find a different Nash equilibrium."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

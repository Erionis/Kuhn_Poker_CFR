{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterfactual Regret Minimization with Kuhn Poker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel Kuhn Poker si sfidano due giocatori con un mazzo composto da tre carte, in ordine di importanza: K (la più alta), Q e J. \n",
    "\n",
    "Ciascun giocatore riceve una carta e posiziona nel piatto una puntata iniziale obbligatoria di 1 chip, detta \"ante\". \n",
    "\n",
    "Successivamente, si svolge un unico turno di puntate durante il quale ogni giocatore ha la possibilità di fare \"bet\" (aggiungendo un ulteriore chip al piatto) o \"check\" (non aggiungendo ulteriori chip al piatto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N_ACTIONS = 2 # 0 = check, 1 = bet\n",
    "N_CARDS = 3 # 0 = jack, 1 = queen, 2 = king\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappresentiamo gli Information Set con la classe InformationSet. \n",
    "\n",
    "\n",
    "Alla fine di ogni iterazione, il metodo self.next_strategy() aggiorna self.strategy_sum, self.strategy e reimposta self.reach_pr. \n",
    "\n",
    "La prossima strategia viene generata nel metodo calc_strategy(). Intuitivamente, calc_strategy() sceglie una strategia proporzionale agli elementi positivi di self.regret_sum. Qualsiasi azione con un regret negativo viene ignorata nella prossima strategia. Se la somma di tutti gli elementi di self.regret_sum è non positiva, allora la prossima strategia consiste nel scegliere casualmente qualsiasi azione in modo uniforme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformationSet():\n",
    "    \"\"\"\n",
    "    Classe che rappresenta un information set.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        # key = history + carta del giocatore\n",
    "        self.key = key\n",
    "        # array contentente la somma dei counterfactual regrets per ogni azione in tutte le visite dell'information set. \n",
    "        # Check = index 0, Bet = index 1\n",
    "        self.regret_sum = np.zeros(N_ACTIONS) \n",
    "        # array contentente la somma della strategie di ogni visita moltiplicata per la reach probability del player corrente\n",
    "        self.strategy_sum = np.zeros(N_ACTIONS) \n",
    "        self.strategy = np.repeat(1/N_ACTIONS, N_ACTIONS) # array contentente la strategia corrente (inizializzata a uniforme)\n",
    "        self.reach_pr = 0 # reach probability del player corrente\n",
    "        self.reach_pr_sum = 0\n",
    "\n",
    "    def next_strategy(self):\n",
    "        \"\"\"\n",
    "        Aggiorna la strategia corrente e la somma delle strategie.\n",
    "        \"\"\"\n",
    "        # aggiorna la somma delle strategie\n",
    "        self.strategy_sum += self.reach_pr * self.strategy\n",
    "        # calcola la nuova strategia con regret matching\n",
    "        self.strategy = self.calc_strategy()\n",
    "        # resetta la reach probability corrente e aggiorna la somma delle reach probability\n",
    "        self.reach_pr_sum += self.reach_pr\n",
    "        self.reach_pr = 0 \n",
    "\n",
    "    def calc_strategy(self):\n",
    "        \"\"\"\n",
    "        Calcola la strategia corrente a partire dai counterfactual regrets.\n",
    "        Sceglie una strategia proporzionale ai agli elementi positivi del regret_sum facendo il Regret Matching.\n",
    "        \"\"\"\n",
    "        \n",
    "        strategy = np.where(self.regret_sum > 0, self.regret_sum, 0) # prende solo i regret positivi\n",
    "\n",
    "        total = sum(strategy) # somma dei regret positivi\n",
    "        if total > 0: \n",
    "            strategy = strategy / total # normalizza i regret positivi\n",
    "        else:\n",
    "            n = N_ACTIONS\n",
    "            strategy = np.repeat(1/n, n) # se non ci sono regret positivi, sceglie una strategia uniforme\n",
    "\n",
    "        return strategy\n",
    "\n",
    "    def get_average_strategy(self):\n",
    "        \"\"\"\n",
    "        Calcola la strategia media su tutte le iterazioni. Questa è la strategia di Nash.\n",
    "        \"\"\"\n",
    "        strategy = self.strategy_sum / self.reach_pr_sum\n",
    "        # Rimuove le strategie trascuraibili\n",
    "        strategy = np.where(strategy < 0.001, 0, strategy)\n",
    "        # Rinormalizza\n",
    "        total = sum(strategy)\n",
    "        strategy /= total\n",
    "\n",
    "        return strategy\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        strategies = ['{:03.2f}'.format(x)\n",
    "                      for x in self.get_average_strategy()]\n",
    "        return '{} {}'.format(self.key.ljust(6), strategies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def card_str(card):\n",
    "    \"\"\"\n",
    "    Ritorna una string representation delle carte.\n",
    "    \"\"\"\n",
    "    if card == 0:\n",
    "        return \"J\"\n",
    "    elif card == 1:\n",
    "        return \"Q\"\n",
    "    return \"K\"\n",
    "\n",
    "\n",
    "def get_info_set(i_map, card, history):\n",
    "    \"\"\"\n",
    "    Ritorna l'information set associato alla carta e alla history.\n",
    "    Se non esiste, allora lo crea e lo aggiunge al dizionario.\n",
    "    \"\"\"\n",
    "    # La chiave è composta da una stringa che rappresenta la carta e la history\n",
    "    key = card_str(card) + \" \" + history\n",
    "    info_set = None\n",
    "    \n",
    "    # Se la chiave non è presente nel dizionario, allora creo un nuovo information set\n",
    "    if key not in i_map:\n",
    "        # Creo un'istanza della classe InformationSet\n",
    "        info_set = InformationSet(key)\n",
    "        # Aggiungo il nuovo information set al dizionario\n",
    "        i_map[key] = info_set\n",
    "        return info_set\n",
    "    \n",
    "    # Altrimenti ritorno l'information set associato alla chiave\n",
    "    return i_map[key]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione cfr() esegue una visita in profondità ricorsiva lungo l'albero del gioco attraversandolo interamente e restituisce l'expected value di quella iterazione. \n",
    "\n",
    "A seconda della tipologia di nodo (history) data in ingresso svolge attività differenti. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cfr(i_map, history=\"\", card_1=-1, card_2=-1, pr_1=1, pr_2=1, pr_c=1):\n",
    "    \"\"\"\n",
    "    Counterfactual regret minimization algorithm.\n",
    "\n",
    "    Parameteri\n",
    "    ----------\n",
    "    i_map: dizionario degli information set\n",
    "    history : [{'r', 'c', 'b'}], stringa che rappresenta il path preso nel game tree\n",
    "        'r': random chance action\n",
    "        'c': check action\n",
    "        'b': bet action\n",
    "    card_1 : carta del player 1\n",
    "    card_2 : carta del player 2\n",
    "    pr_1 : Probabilità che il player 1 arrivi a history\n",
    "    pr_2 : Probabilità che il player 2 arrivi a history\n",
    "    pr_c: Contributo di probabilità del chance node per raggiungere history\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Se l'history è un chance node, allora ritorna il valore atteso \n",
    "    if is_chance_node(history):\n",
    "        return chance_util(i_map)\n",
    "    \n",
    "    # Se l'history è un terminal node, allora ritorna la terminal utility di questa combinazione di carte\n",
    "    if is_terminal(history):\n",
    "        return terminal_util(history, card_1, card_2)\n",
    "\n",
    "    \n",
    "    # --------------Se arrivo qui, allora l'history è un decision node----------------\n",
    "    \n",
    "    # Calcolo il numero di azioni che sono state prese fino ad ora\n",
    "    n = len(history)    \n",
    "    # Se n è pari, allora è il turno del player 1, altrimenti è il turno del player 2\n",
    "    is_player_1 = n % 2 == 0\n",
    "    # Deduco l'infromation set della history del player corrente\n",
    "    info_set = get_info_set(i_map, card_1 if is_player_1 else card_2, history)\n",
    "    # Deduco la strategia corrente del player corrente,calcolata all'iterazione precedente\n",
    "    strategy = info_set.strategy\n",
    "    # Aggiungo la reach probability del player corrente all'information set\n",
    "    if is_player_1:\n",
    "        info_set.reach_pr += pr_1 # \n",
    "    else:\n",
    "        info_set.reach_pr += pr_2\n",
    "\n",
    "    # array delle Counterfactual utility per azione\n",
    "    action_utils = np.zeros(N_ACTIONS)\n",
    "\n",
    "    # Chiamo ricorsivamente cfr per ogni azione possibile  \n",
    "    for i, action in enumerate([\"c\", \"b\"]):\n",
    "        # genero il prossimo nodo dell'history\n",
    "        next_history = history + action\n",
    "        # ogni player sceglie l'action a con probabilità strategy[i] che deve essere moltipliata per la reach probability del player corrente\n",
    "        if is_player_1:\n",
    "            # moltiplico per -1 perchè cfr ritorna l'utility per il prossimo turno\n",
    "            action_utils[i] = -1 * cfr(i_map= i_map, history= next_history,\n",
    "                                       card_1= card_1, card_2= card_2,\n",
    "                                       pr_1= pr_1 * strategy[i], pr_2= pr_2, pr_c= pr_c) \n",
    "        else:\n",
    "            action_utils[i] = -1 * cfr(i_map = i_map, history= next_history,\n",
    "                                       card_1= card_1,card_2= card_2,\n",
    "                                       pr_1= pr_1,pr_2= pr_2 * strategy[i],pr_c= pr_c)\n",
    "\n",
    "    # Calcolo gli expected utility su tutte le azioni\n",
    "    expected_value = sum(action_utils * strategy) \n",
    "    # calcolo i counterfactual regrets\n",
    "    regrets = action_utils - expected_value \n",
    "    # Aggiorno i counterfactual regrets dell'information set\n",
    "    if is_player_1:\n",
    "        info_set.regret_sum += pr_2 * pr_c * regrets\n",
    "    else:\n",
    "        info_set.regret_sum += pr_1 * pr_c * regrets\n",
    "    \n",
    "    return expected_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione is_chance_node() determina se ci troviamo in un nodo di chance controllando se la history è vuota.\n",
    "\n",
    "Se restituisce true, allora chance_util() enumera tutte le 6 possibili combinazioni di mani generate dal chance node. Per ogni possibilità, richiamiamo in modo ricorsivo cfr() considerando che nessun giocatore ha preso alcuna azione, quindi le loro reach probability sono entrambe 1. Ogni combinazione di mani ha una probabilità uniforme. LA funzione ritorna il valore atteso su tutte le combinazioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chance_node(history):\n",
    "    return history == \"\" # return True if history == \"\" else False\n",
    "\n",
    "# se sono in un chance node \n",
    "def chance_util(i_map):\n",
    "    expected_value = 0\n",
    "    n_possibilities = 6\n",
    "    # 6 possibili combinazioni di carte\n",
    "    for i in range(N_CARDS):\n",
    "        for j in range(N_CARDS):\n",
    "            if i != j:\n",
    "                # ottengo la somma dell'ev di ogni combinazione\n",
    "                expected_value += cfr(i_map= i_map,\n",
    "                                      history= \"rr\",\n",
    "                                      card_1= i,\n",
    "                                      card_2= j,\n",
    "                                      pr_1= 1, # probabilità che il giocatore 1 arrivi a quel nodo è 1 perchè ancora non ha giocato\n",
    "                                      pr_2= 1, # probabilità che il giocatore 2 arrivi a quel nodo è 1 perchè ancora non ha giocato\n",
    "                                      pr_c= 1/n_possibilities) # probabilità che il chance node arrivi a quel nodo è 1/6 perchè è uan distribuzione uniforme\n",
    "                \n",
    "    return expected_value/n_possibilities # ritorno la media degli ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione is_terminal() verifica se la history è presente nell'insieme delle history possibili dei terminal nodes. Se True, terminal_util() restituisce l'utility del player corrente. \n",
    "\n",
    "Poiché i giocatori si alternano, il player corrente è 1 se il numero di azioni nella history è pari, altrimenti è il 2. Per calcolare l'utility di una terminal history, ci sono tre casi:\n",
    "\n",
    "- Se l'ultimo giocatore ha fatto \"check\"(fold), il player corrente vince 1 chip.\n",
    "- Se entrambi i giocatori fanno \"check\" durante il round di puntate, si arriva allo showdown e il giocatore con la carta più alta vince 1 chip.\n",
    "- I giocatori arrivano a showdown con un piatto di 4 chip. Il giocatore con la carta più alta vince 2 chip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_terminal(history):\n",
    "    \"\"\"\n",
    "    Ritorna True se l`history` è una terminal history.\n",
    "    \"\"\"    \n",
    "    # 5 possibili terminal history\n",
    "    possibilities = {\"rrcc\": True, # showdown con check e check\n",
    "                     \"rrbb\": True, # showdown con bet e bet(call)                  \n",
    "                     \"rrcbb\": True, # showdown con check, bet e bet(call)\n",
    "                     \"rrbc\": True, # non showdown con bet e check(fold)\n",
    "                     \"rrcbc\": True, # non showdown con check, bet e check(fold)\n",
    "                     }\n",
    "    return history in possibilities\n",
    "\n",
    "\n",
    "def terminal_util(history, card_1, card_2):\n",
    "    \"\"\"\n",
    "    Ritorna l'utility di una terminal history per il player corrente. \n",
    "    Siccome i players is alternano ad ogni turno, se il numero di azioni è pari allora\n",
    "    il player corrente è il player 1 altrimenti è il player 2 \n",
    "    \"\"\"\n",
    "    n = len(history) # numero di azioni fatte\n",
    "    # se il numero di azioni è pari allora è il player 1 altrimenti è il player 2\n",
    "    card_player = card_1 if n % 2 == 0 else card_2 \n",
    "    card_opponent = card_2 if n % 2 == 0 else card_1\n",
    "\n",
    "    # No showdown\n",
    "    if history == \"rrcbc\" or history == \"rrbc\":\n",
    "        # Opponent ha foldato, il player corrente vince 1\n",
    "        return 1\n",
    "    # Showdown senza bets\n",
    "    elif history == \"rrcc\":\n",
    "        # Chi ha la carta più alta vince 1\n",
    "        return 1 if card_player > card_opponent else -1\n",
    "\n",
    "    # Showdown con bet e call\n",
    "    if history == \"rrcbb\" or history == \"rrbb\":\n",
    "        # Chi ha la carta più alta vince 2\n",
    "        return 2 if card_player > card_opponent else -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(ev, i_map):\n",
    "    print('player 1 expected value: {}'.format(ev))\n",
    "    print('player 2 expected value: {}'.format(-1 * ev))\n",
    "\n",
    "    print()\n",
    "    print('player 1 strategies:')\n",
    "    print(f\"History  Check    Bet\")\n",
    "    sorted_items = sorted(i_map.items(), key=lambda x: x[0])\n",
    "    for _, v in filter(lambda x: len(x[0]) % 2 == 0, sorted_items):\n",
    "        print(v)\n",
    "    print()\n",
    "    print('player 2 strategies:')\n",
    "    print(f\"History  Check    Bet\")\n",
    "    for _, v in filter(lambda x: len(x[0]) % 2 == 1, sorted_items):\n",
    "        print(v)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ogni iterazione di cfr() restituisce il valore atteso del gioco per quella specifica iterazione. Siamo in grado di approssimare il vero valore del gioco facendo la media su tutte le iterazioni. Il vero valore del gioco rappresenta l'importo atteso che un giocatore vincerà seguendo l' average strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 expected value: -0.05666979368427769\n",
      "player 2 expected value: 0.05666979368427769\n",
      "\n",
      "player 1 strategies:\n",
      "History  Check    Bet\n",
      "J rr   ['0.79', '0.21']\n",
      "J rrcb ['1.00', '0.00']\n",
      "K rr   ['0.39', '0.61']\n",
      "K rrcb ['0.00', '1.00']\n",
      "Q rr   ['1.00', '0.00']\n",
      "Q rrcb ['0.45', '0.55']\n",
      "\n",
      "player 2 strategies:\n",
      "History  Check    Bet\n",
      "J rrb  ['1.00', '0.00']\n",
      "J rrc  ['0.67', '0.33']\n",
      "K rrb  ['0.00', '1.00']\n",
      "K rrc  ['0.00', '1.00']\n",
      "Q rrb  ['0.66', '0.34']\n",
      "Q rrc  ['1.00', '0.00']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i_map = {}   # dizionario degli information set\n",
    "n_iterations = 10000 # numero di iterazioni\n",
    "expected_game_value = 0\n",
    "\n",
    "for _ in range(n_iterations): # run CFR algorithm n_iterations times\n",
    "    # update game value\n",
    "    expected_game_value += cfr(i_map) # update game value\n",
    "    # update strategies per ogni information set\n",
    "    for _, infoset in i_map.items():\n",
    "        infoset.next_strategy() \n",
    "\n",
    "expected_game_value /= n_iterations\n",
    "\n",
    "display_results(expected_game_value, i_map) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il game value del Kuhn poker è di 1/18= 0.0555. Gia dopo 10000 iterazioni si nota la convergenza al game value. Al crescere del numero di iterazioni l'average total counterfactual regret converge verso 0 mentre l' avarage strategy converge al Nash equilibium.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
